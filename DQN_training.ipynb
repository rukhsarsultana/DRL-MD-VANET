{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from statistics import mode\n",
    "import time\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from memory_profiler import profile\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of available GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if not gpus:\n",
    "    print(\"No GPUs found. TensorFlow is using CPU.\")\n",
    "else:\n",
    "    for gpu in gpus:\n",
    "        print(\"GPU device name:\", gpu.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read input combined dataset for DQN training\n",
    "sdata = pd.read_csv('DQN_Dataset_Sample/DQN_Training_Data_Sample/CombinedAttackData.csv',index_col=0)\n",
    "# Show input data\n",
    "sdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show input data columns\n",
    "sdata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate lower bound and upper bound values for average speed at each instant\n",
    "sdata['ASLB'] = sdata['TSmean']-sdata['TSstd']\n",
    "sdata['ASUB'] = sdata['TSmean']+sdata['TSstd']\n",
    "#Calculate difference of two types of calculated distances \n",
    "sdata['DisDiff'] = abs(abs(sdata['disCover'])- abs(sdata['SADis']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop additional columns from input data which is not required for further processing\n",
    "sdata = sdata.drop(columns=['rTotalSpeed', 'sxspeed', 'syspeed',  'syacc', 'sTotalAcc', 'disCover','SADis', 'TSmean', 'TSstd']).reset_index(drop=True)\n",
    "sdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the data # sdata\n",
    "std_columns = ['sTotalSpeed','sxacc', 'beaconRate', 'rDensity', 'DisDiff', 'rAvgSpeed']\n",
    "\n",
    "# Extract the columns to be standardized\n",
    "data_to_standardize = sdata[std_columns]\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform the data\n",
    "scaled_data = scaler.fit_transform(data_to_standardize)\n",
    "\n",
    "# Create a new DataFrame with the standardized data\n",
    "df_standardized = pd.DataFrame(scaled_data, columns=[f'{col}_std' for col in std_columns])\n",
    "\n",
    "# Concatenate the new standardized columns with the original DataFrame\n",
    "sdata = pd.concat([sdata, df_standardized], axis=1)\n",
    "\n",
    "sdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom callback to measure training time and memory usage\n",
    "class TimeMemoryCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.data = {'Epoch': [], 'Training Time': [], 'Memory RSS': [], 'Memory VMS': []}\n",
    "        self.start_time = 0\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Memory profiling for the beginning of each epoch\n",
    "        memory_info = self.get_memory_usage()\n",
    "        self.data['Epoch'].append(epoch)\n",
    "        self.data['Memory RSS'].append(memory_info['rss'])\n",
    "        self.data['Memory VMS'].append(memory_info['vms'])\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - self.start_time\n",
    "        self.data['Training Time'] = [elapsed_time] * len(self.data['Epoch'])\n",
    "\n",
    "    @staticmethod\n",
    "    @profile\n",
    "    def get_memory_usage():\n",
    "        # This method is decorated with @profile to enable memory profiling\n",
    "        import psutil\n",
    "\n",
    "        process = psutil.Process()\n",
    "        memory_info = process.memory_info()\n",
    "\n",
    "        return {'rss': memory_info.rss / (1024 ** 2), 'vms': memory_info.vms / (1024 ** 2)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DQN model\n",
    "state_size = 5\n",
    "action_size = 2  # Accept or Reject\n",
    "learning_rate = 0.001\n",
    "epsilon = 1\n",
    "decoy_rate = 0.99\n",
    "epsilon_min = 0.1\n",
    "gamma = 0.001\n",
    "epochs = 10\n",
    "max_steps = 1000\n",
    "# Initialize Q-table with zeros without heading\n",
    "num_states = len(sdata)\n",
    "Q = np.zeros((num_states, action_size))\n",
    "\n",
    "# Create a Q-network and move it to the GPU\n",
    "# if not gpus:\n",
    "model = keras.Sequential([keras.layers.Dense(64, activation='relu', input_shape=(state_size,)),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(action_size, activation='linear')])\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate), loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the experience replay buffer\n",
    "buffer_size = 5000\n",
    "per = 0\n",
    "\n",
    "# Initialize an empty list to store the loss values, accuracy and rewards\n",
    "episode_train_loss = []\n",
    "episode_test_loss = []\n",
    "episode_train_accuracy_model = []\n",
    "episode_train_accuracy_Calculated = []\n",
    "episode_detection_rate = []\n",
    "each_reward = []\n",
    "episode_reward = []\n",
    "\n",
    "#Initialize Q table to store Q values\n",
    "Q_table = pd.DataFrame(columns=['State_ID', 'Q0', 'Q1'])\n",
    "\n",
    "#Initialize dataframe for every loss and accuracy\n",
    "all_loss_df = pd.DataFrame(columns=['Each_Loss', 'Each_Accuracy'])\n",
    "state_id = 0\n",
    "#count for epsilon decays\n",
    "ecount = 0\n",
    "nid = state_id\n",
    "batch_size = 32 # Adjust as needed\n",
    "\n",
    "rnum = len(sdata.receiver.unique())  #number of unique receivers\n",
    "\n",
    "with tf.device(\"/GPU:0\"):\n",
    "  test_tp, test_fp, test_tn, test_fn = 0, 0, 0, 0\n",
    "  all_results = pd.DataFrame(columns = ['epoch','TP', 'TN', 'FP', 'FN', 'Accuracy', 'Precision', 'Recall', 'Fscore', 'FPR', 'MDR'])\n",
    "  overall_overhead = pd.DataFrame(columns = ['Epoch', 'Episode', 'Step','Receiver', 'Memory_Usage', 'Time_Overhead'])\n",
    "  acc1, pre1, recall1, fs1, fpr1, mdr1 = 0, 0, 0, 0, 0, 0\n",
    "  acc, pre, recall, fs, fpr, mdr = 0, 0, 0, 0, 0, 0\n",
    "  for epoch in range(0,10):\n",
    "\n",
    "      rcount = 0\n",
    "      all_overhead = pd.DataFrame(columns = ['Epoch', 'Episode', 'Step', 'Receiver', 'Memory_Usage', 'Time_Overhead'])\n",
    "\n",
    "      for ri in sdata.receiver.unique():\n",
    "        rcount+=1\n",
    "        rdata = sdata[sdata.receiver==ri].sort_values(by='newTime').reset_index(drop=True)\n",
    "\n",
    "        experience_replay = []\n",
    "        train_loss = 0\n",
    "        train_accuracy = 0\n",
    "        batch_count = 0\n",
    "        reward_total = 0\n",
    "        for si in range(len(rdata)):\n",
    "          #Extract state vector\n",
    "          state = [rdata.loc[si,'beaconRate_std'],rdata.loc[si,'rDensity_std'], rdata.loc[si,'sTotalSpeed_std'], abs(rdata.loc[si,'sxacc_std']), rdata.loc[si,'DisDiff_std'], rdata.loc[si,'rAvgSpeed_std']]\n",
    "\n",
    "          #Choose action through exploration or exploitation\n",
    "          if np.random.rand() < epsilon:\n",
    "            action = random.choice([0,1])\n",
    "            epsilon = epsilon*decoy_rate\n",
    "            if epsilon >= epsilon_min:\n",
    "              epsilon = epsilon*decoy_rate\n",
    "           \n",
    "            ecount+=1\n",
    "            \n",
    "          else:\n",
    "            q_values = model.predict(np.array([state]))\n",
    "            action = np.argmax(q_values)\n",
    "            Q[nid,:] = q_values\n",
    "\n",
    "            Q_table.loc[len(Q_table.index)] = [nid, q_values[0][0], q_values[0][1]]\n",
    "\n",
    "          #Compute Reward\n",
    "          actual_action = rdata.loc[si,'AttackerType']\n",
    "          if rdata.loc[si,'ASLB'] < rdata.loc[si,'sTotalSpeed'] <= rdata.loc[si,'ASUB'] and rdata.loc[si,'DisDiff']==0:\n",
    "            rflag = 1\n",
    "          else:\n",
    "            rflag = 0\n",
    "\n",
    "          if rflag == 1:\n",
    "            reward = 1 if action == 0 else -1\n",
    "          else:\n",
    "            reward = -1 if action ==  0 else 1\n",
    "\n",
    "          if actual_action == action:\n",
    "            if action == 0:\n",
    "              test_tn+=1\n",
    "            else:\n",
    "              test_tp+=1\n",
    "          else:\n",
    "            if action == 0:\n",
    "              test_fn+=1\n",
    "            else:\n",
    "              test_fp+=1\n",
    "\n",
    "          each_reward.append(reward)\n",
    "          reward_total+=reward\n",
    "\n",
    "          #Store to experiance replay\n",
    "          experience_replay.append((state, action, reward))\n",
    "\n",
    "    #     # Batch Sampling and Q-Value Updates\n",
    "\n",
    "          if si>=5:\n",
    "            if len(experience_replay) >= 32:\n",
    "              batch_size = 32\n",
    "            else:\n",
    "              batch_size = si-1\n",
    "            batch_count += 1\n",
    "            minibatch = random.sample(experience_replay, batch_size)  # Use random.sample to get a sublist of experiences\n",
    "            states, actions, rewards = zip(*minibatch)\n",
    "\n",
    "            # Convert the tuples to numpy arrays\n",
    "            states = np.array(states)\n",
    "            actions = np.array(actions)\n",
    "            rewards = np.array(rewards)\n",
    "                  \n",
    "            # Calculate the Q-values for the current state\n",
    "            q_values = model.predict(states)\n",
    "\n",
    "            # Calculate the target Q-values based on the current Q-values and rewards\n",
    "            target_q_values = q_values.copy()\n",
    "\n",
    "            # Calculate the target Q-values for the minibatch\n",
    "            for k in range(batch_size):\n",
    "              target_q_values[k][actions[k]] = rewards[k] + gamma * np.max(q_values[k])\n",
    "\n",
    "            # Update the Q-values using the DQN loss function\n",
    "\n",
    "            # Create the custom callback\n",
    "            custom_callback = TimeMemoryCallback()\n",
    "            history = model.fit(states, target_q_values, epochs=1, verbose=0, callbacks=[custom_callback])\n",
    "            train_loss += history.history['loss'][0]\n",
    "            train_accuracy += history.history['accuracy'][0]\n",
    "            print(\"batch count=\", batch_count)\n",
    "            print(\"loss and accuracy=\", history.history['loss'][0],history.history['accuracy'][0])\n",
    "            all_loss_df.loc[len(all_loss_df.index)] = [history.history['loss'][0],history.history['accuracy'][0]]\n",
    "\n",
    "            episode_train_loss.append(train_loss/batch_count)\n",
    "            episode_train_accuracy_Calculated.append((test_tp+test_tn)/(test_tp+test_fp+test_tn+test_fn))\n",
    "            episode_train_accuracy_model.append(train_accuracy/batch_count)\n",
    "            episode_detection_rate.append((test_tp)/(test_tp + test_fn))\n",
    "            episode_reward.append(reward_total/batch_count)\n",
    "\n",
    "            if (test_tp+test_fp)!=0:\n",
    "              pre = test_tp/(test_tp+test_fp)\n",
    "            if (test_tp+test_fn)!=0:\n",
    "              recall = test_tp/(test_tp+test_fn)\n",
    "            if (test_tn+test_fp)!=0:\n",
    "              fpr = test_fp/(test_tn+test_fp)\n",
    "            if (test_tp+test_fn)!=0:\n",
    "              mdr = test_fn/(test_tp+test_fn)\n",
    "              acc = (test_tp+test_tn)/(test_tp+test_fp+test_tn+test_fn)\n",
    "            if pre:\n",
    "              if recall:\n",
    "                fs = 2/((1/pre)+(1/recall))\n",
    "            all_results.loc[len(all_results.index)] = [epoch, test_tp, test_tn, test_fp, test_fn, acc, pre, recall, fs, fpr, mdr]\n",
    "\n",
    "            # Convert the collected data to a DataFrame\n",
    "            df_metrics = pd.DataFrame(custom_callback.data)\n",
    "\n",
    "            all_overhead.loc[len(all_overhead)] = [epoch, rcount, ri, si, df_metrics['Memory RSS'], df_metrics['Training Time']]\n",
    "\n",
    "        overall_overhead.loc[len(overall_overhead)] = [all_overhead.loc[len(all_overhead)-1,'Epoch'], all_overhead.loc[len(all_overhead)-1,'Episode'],all_overhead.loc[len(all_overhead)-1,'Step'], all_overhead.loc[len(all_overhead)-1,'Receiver'], all_overhead.loc[len(all_overhead)-1,'Memory_Usage'], all_overhead.loc[len(all_overhead)-1,'Time_Overhead']]\n",
    "\n",
    "      model.save(f\"Training_output/Trained_{epoch+1}e_103L_001DF_Model.h5\")  # Save the model to a file\n",
    "\n",
    "      print(\"Epoch done=\", epoch)\n",
    "      if (per+1)*epochs <= epoch*1000:\n",
    "          per += 1\n",
    "          print(round(per/10,1), '% done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the overhead, Q values and training results\n",
    "overall_overhead.to_csv('Training_output/Training_103L_001DF_Overhead2.csv')\n",
    "Q_table.to_csv('Training_output/Training_103L_001DF_QValueEach2.csv')\n",
    "all_results.to_csv('Training_output/Training_103L_001DF_Results2.csv')\n",
    "\n",
    "result_data = {'Loss': episode_train_loss, 'Model_accuracy': episode_train_accuracy_model, 'Calculated_accuracy': episode_train_accuracy_Calculated, 'Detection_rate': episode_detection_rate, 'Reward': episode_reward }\n",
    "df_result = pd.DataFrame(result_data)\n",
    "df_result.to_csv('Training_output/Training_103L_001DF_TrainingResults2.csv')\n",
    "\n",
    "csv_file_path = 'Training_output/Training_103L_001DF_QTable2.csv'\n",
    "\n",
    "#Save Q table\n",
    "# Open the CSV file in write mode\n",
    "with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "    # Create a CSV writer object\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "\n",
    "    # Write each row of the array to the CSV file\n",
    "    for row in Q:\n",
    "        csv_writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# Specify the file path\n",
    "csv_file_path = 'STD_Training/STD_AllAttacksTT_103L_001DF_QTable2.csv'\n",
    "\n",
    "# Open the CSV file in write mode\n",
    "with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "    # Create a CSV writer object\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "\n",
    "    # Write each row of the array to the CSV file\n",
    "    for row in Q:\n",
    "        csv_writer.writerow(row)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
