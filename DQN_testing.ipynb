{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from statistics import mode\n",
    "import time\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from memory_profiler import profile\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from math import isqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of available GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if not gpus:\n",
    "    print(\"No GPUs found. TensorFlow is using CPU.\")\n",
    "else:\n",
    "    for gpu in gpus:\n",
    "        print(\"GPU device name:\", gpu.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('Training_output/Trained_10e_103L_001DF_Model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize inputs and hyper parameters\n",
    "state_size = 5\n",
    "action_size = 2  # Accept or Reject\n",
    "learning_rate = 0.001\n",
    "epsilon = 1\n",
    "decoy_rate = 0.99\n",
    "epsilon_min = 0.1\n",
    "gamma = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize dataframes for outputs\n",
    "epoch_train_accuracy_Calculated = []\n",
    "detection_rate = []\n",
    "each_reward = []\n",
    "epoch_reward = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom callback to measure training time and memory usage\n",
    "class TimeMemoryCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.data = {'Epoch': [], 'Training Time': [], 'Memory RSS': [], 'Memory VMS': []}\n",
    "        self.start_time = 0\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Memory profiling for the beginning of each epoch\n",
    "        memory_info = self.get_memory_usage()\n",
    "        self.data['Epoch'].append(epoch)\n",
    "        self.data['Memory RSS'].append(memory_info['rss'])\n",
    "        self.data['Memory VMS'].append(memory_info['vms'])\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - self.start_time\n",
    "        self.data['Training Time'] = [elapsed_time] * len(self.data['Epoch'])\n",
    "\n",
    "    @staticmethod\n",
    "    @profile\n",
    "\n",
    "    def get_memory_usage():\n",
    "        # This method is decorated with @profile to enable memory profiling\n",
    "        import psutil\n",
    "\n",
    "        process = psutil.Process()\n",
    "        memory_info = process.memory_info()\n",
    "\n",
    "        return {'rss': memory_info.rss / (1024 ** 2), 'vms': memory_info.vms / (1024 ** 2)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for standardizing data\n",
    "def stdInput(data):\n",
    "    std_columns = ['sTotalSpeed','sxacc', 'beaconRate', 'DisDiff', 'rAvgSpeed']\n",
    "\n",
    "# Extract the columns to be standardized\n",
    "    data_to_standardize = data[std_columns]\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform the data\n",
    "    scaled_data = scaler.fit_transform(data_to_standardize)\n",
    "\n",
    "# Create a new DataFrame with the standardized data\n",
    "    df_standardized = pd.DataFrame(scaled_data, columns=[f'{col}_std' for col in std_columns])\n",
    "\n",
    "# Concatenate the new standardized columns with the original DataFrame\n",
    "    data = pd.concat([data, df_standardized], axis=1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_id = 0\n",
    "#count for epsilon decays\n",
    "ecount = 0\n",
    "nid = state_id\n",
    "\n",
    "cum_results = pd.DataFrame(columns = ['Attack','TP', 'TN', 'FP', 'FN', 'Accuracy', 'Precision', 'Recall', 'Fscore', 'FPR', 'MDR'])\n",
    "all_each_results = pd.DataFrame(columns = ['Attack','TP', 'TN', 'FP', 'FN', 'Accuracy', 'Precision', 'Recall', 'Fscore', 'FPR', 'MDR'])\n",
    "\n",
    "\n",
    "tp, fp, tn, fn = 0, 0, 0, 0\n",
    "acc1, pre1, recall1, fs1, fpr1, mdr1 = 0, 0, 0, 0, 0, 0\n",
    "\n",
    "with tf.device(\"/GPU:0\"):\n",
    "  for filetype in ['ConstPos', 'ConstPosOffset', 'RandomPos', 'RandomPosOffset', 'ConstSpeed', 'ConstSpeedOffset', 'RandomSpeed', 'RandomSpeedOffset', 'EventualStop', 'DelayedMessages', 'DoS', 'DoSRandom', 'DoSDisruptive', 'DataReplay', 'Disruptive', 'DataReplaySybil', 'GridSybil', 'DoSRandomSybil', 'DoSDisruptiveSybil']:\n",
    "\n",
    "      #Read attack dataset in above sequence for testing\n",
    "      sdata = pd.read_csv(f'DQN_Dataset_Sample/DQN_Testing_Data_Sample/Pair_CombinedSec_{filetype}.csv')\n",
    "      sdata['DisDiff'] = abs(abs(sdata['SADis'])-abs(sdata['disCover']))\n",
    "      sdata['ASLB'] = sdata['TSmean']- sdata['TSstd']\n",
    "      sdata['ASUB'] = sdata['TSmean']+ sdata['TSstd']\n",
    "      \n",
    "\n",
    "      #Standardize the input data\n",
    "      sdata = stdInput(sdata)\n",
    "      \n",
    "      test_tp, test_fp, test_tn, test_fn = 0, 0, 0, 0\n",
    "\n",
    "      acc, pre, recall, fs, fpr, mdr = 0, 0, 0, 0, 0, 0\n",
    "\n",
    "\n",
    "      #first find results in terms of accuracy, precision, recall, f1score and FPR\n",
    "      each_results = pd.DataFrame(columns = ['TP', 'TN', 'FP', 'FN', 'Accuracy', 'Precision', 'Recall', 'Fscore', 'FPR', 'MDR'])\n",
    "      \n",
    "      urcvr = sdata.receiver.unique()\n",
    "      Speed_buffer = pd.DataFrame(columns=['Receiver','Speed', 'Mean', 'STD', 'Exploit/Update'])\n",
    "      all_overhead = pd.DataFrame(columns = ['RCount', 'MCount', 'Receiver', 'Memory_Usage', 'Time_Overhead'])\n",
    "      rcount = 0\n",
    "      for ri in urcvr:\n",
    "        rdata = sdata[sdata.receiver==ri].sort_values(by='newTime').reset_index(drop=True)\n",
    "        rcount+=1\n",
    "        \n",
    "        experience_replay = []\n",
    "        for si in range(len(rdata)):\n",
    "        # Replace with actual state data\n",
    "          state = [rdata.loc[si,'beaconRate_std'],rdata.loc[si,'sTotalSpeed_std'], abs(rdata.loc[si,'sxacc_std']), rdata.loc[si,'DisDiff_std'], rdata.loc[si,'rAvgSpeed_std']]\n",
    "\n",
    "          if si==0:\n",
    "            Speed_buffer.loc[len(Speed_buffer)] = [rdata.loc[si,'receiver'], rdata.loc[si,'rAvgSpeed'],rdata.loc[si,'rAvgSpeed'], rdata.loc[si,'rAvgSpeed'], 'Exploit']\n",
    "          else:\n",
    "            # Calculate mean and std\n",
    "            if si<5:\n",
    "              wind = si\n",
    "            else:\n",
    "              wind = 10\n",
    "            mean_value = rdata.loc[0:si,'rAvgSpeed'].shift().rolling(window=wind).mean().iloc[-1]\n",
    "            std_value = rdata.loc[0:si,'rAvgSpeed'].shift().rolling(window=wind).std().iloc[-1]\n",
    "            \n",
    "            Speed_buffer.loc[len(Speed_buffer)] = [rdata.loc[si,'receiver'], rdata.loc[si,'rAvgSpeed'], mean_value, std_value+1, 'Exploit']\n",
    "\n",
    "          \n",
    "        # Check whether to test or update the model\n",
    "\n",
    "          if Speed_buffer.loc[len(Speed_buffer)-1,'Mean']- Speed_buffer.loc[len(Speed_buffer)-1,'STD'] <= Speed_buffer.loc[len(Speed_buffer)-1,'Speed'] <= Speed_buffer.loc[len(Speed_buffer)-1,'Mean']+ Speed_buffer.loc[len(Speed_buffer)-1,'STD']:\n",
    "            q_values = model.predict(np.array([state]))\n",
    "            action = np.argmax(q_values)        \n",
    "\n",
    "          else:\n",
    "            if si<5:\n",
    "              q_values = model.predict(np.array([state]))\n",
    "              action = np.argmax(q_values)\n",
    "\n",
    "            else:\n",
    "              Speed_buffer.loc[len(Speed_buffer)-1, 'Exploit/Update'] = 'Update'\n",
    "              batch_size = len(experience_replay) if len(experience_replay)<32 else 32\n",
    "\n",
    "              if batch_size>1:\n",
    "                minibatch = experience_replay[-batch_size:]\n",
    "                states, actions, rewards = zip(*minibatch)\n",
    "\n",
    "                # Convert the tuples to numpy arrays\n",
    "                states = np.array(states)\n",
    "                actions = np.array(actions)\n",
    "                rewards = np.array(rewards)\n",
    "\n",
    "                q_values = model.predict(states)\n",
    "\n",
    "                # Calculate the target Q-values based on the current Q-values and rewards\n",
    "                target_q_values = q_values.copy()\n",
    "\n",
    "                for k in range(batch_size):\n",
    "                  target_q_values[k][int(actions[k])] = rewards[k] + gamma * np.max(q_values[k])\n",
    "\n",
    "                # Create the custom callback\n",
    "                custom_callback = TimeMemoryCallback()\n",
    "                # Update the Q-values using the DQN loss function\n",
    "                \n",
    "                # Example modification of the output layer\n",
    "                # Modify the output layer for the new action size\n",
    "                model.layers[-1] = keras.layers.Dense(action_size, activation='linear')\n",
    "\n",
    "                # Optionally, freeze some or all pre-trained layers\n",
    "                for layer in model.layers[:-1]:\n",
    "                    layer.trainable = False\n",
    "\n",
    "                # Compile the model for the new task\n",
    "                model.compile(optimizer=keras.optimizers.Adam(0.2), loss='mse', metrics=['accuracy'])\n",
    "\n",
    "                model.fit(states, target_q_values, epochs=2, verbose=0, callbacks=[custom_callback])\n",
    "\n",
    "\n",
    "              q_values = model.predict(np.array([state]))\n",
    "              action = np.argmax(q_values) \n",
    "\n",
    "              # Convert the collected data to a DataFrame\n",
    "              df_metrics = pd.DataFrame(custom_callback.data)\n",
    "              all_overhead.loc[len(all_overhead)] = [rcount, ri, si, df_metrics['Memory RSS'], df_metrics['Training Time']]\n",
    "\n",
    "        actual_action = rdata.loc[si,'AttackerType']\n",
    "            \n",
    "        if (rdata.loc[si,'ASLB'] <= rdata.loc[si,'sTotalSpeed'] < rdata.loc[si,'ASUB']) and rdata.loc[si,'DisDiff']==0:\n",
    "          rflag = 1\n",
    "        else:\n",
    "          rflag = 0\n",
    "\n",
    "        rlag=1 if actual_action == 0 else 0\n",
    "        if rflag == 1:\n",
    "          reward = 1 if action == 0 else -1\n",
    "        else:\n",
    "          reward = -1 if action ==  0 else 1 \n",
    "\n",
    "        if actual_action == action:\n",
    "          if action == 0:\n",
    "            test_tn+=1\n",
    "          else:\n",
    "            test_tp+=1\n",
    "        else:\n",
    "          if action == 0:\n",
    "            test_fn+=1\n",
    "          else:\n",
    "            test_fp+=1\n",
    "\n",
    "        each_reward.append(reward)\n",
    "    #         experience_replay.append((state, action, reward, target_q_value))\n",
    "        experience_replay.append((state, action, reward))\n",
    "    \n",
    "        if (test_tp+test_fp)!=0:\n",
    "          pre = test_tp/(test_tp+test_fp)\n",
    "        if (test_tp+test_fn)!=0:\n",
    "          recall = test_tp/(test_tp+test_fn)\n",
    "                  # tpr = tp/(tp+fn)\n",
    "        if (test_tn+test_fp)!=0:\n",
    "          fpr = test_fp/(test_tn+test_fp)\n",
    "        if (test_tp+test_fn)!=0:\n",
    "          mdr = test_fn/(test_tp+test_fn)\n",
    "        acc = (test_tp+test_tn)/(test_tp+test_fp+test_tn+test_fn)\n",
    "        if pre:\n",
    "          if recall:\n",
    "            fs = 2/((1/pre)+(1/recall))\n",
    "        each_results.loc[len(each_results.index)] = [test_tp, test_tn, test_fp, test_fn, acc, pre, recall, fs, fpr, mdr]\n",
    "      \n",
    "      each_results.to_csv(f'Testing_output/Testing_103L_001DF_Results_{filetype}.csv') \n",
    "      Speed_buffer.to_csv(f'Testing_output/Testing_103L_001DF_SpeedBuffer_{filetype}.csv')\n",
    "      all_overhead.to_csv(f'Testing_output/Testing_103L_001DF_Overhead_{filetype}.csv')\n",
    "\n",
    "      all_each_results.loc[len(all_each_results.index)] = [filetype, test_tp, test_tn, test_fp, test_fn, acc, pre, recall, fs, fpr, mdr]\n",
    "      tp+=test_tp\n",
    "      tn+=test_tn\n",
    "      fp+=test_fp\n",
    "      fn+=test_fn     \n",
    "        \n",
    "      acc1 = (tp+tn)/(tp+tn+fp+fn)\n",
    "      if (tp+fp)!=0:\n",
    "        pre1 = tp/(tp+fp)\n",
    "      if (tp+fn)!=0:\n",
    "        recall1 = tp/(tp+fn)\n",
    "      \n",
    "      if (tn+fp)!=0:\n",
    "        fpr1 = fp/(tn+fp)\n",
    "      else:\n",
    "        fpr1 = 0\n",
    "      if (tp+fn)!=0:\n",
    "        mdr1 = fn/(tp+fn)\n",
    "\n",
    "      if pre1:\n",
    "        if recall1:\n",
    "          fs1 = 2/((1/pre1)+(1/recall1))\n",
    "\n",
    "      cum_results.loc[len(cum_results.index)] = [filetype, tp, tn, fp, fn, acc1, pre1, recall1, fs1, fpr1, mdr1]\n",
    "      model.save(f\"Testing_output/Testing_103L_001DF_{filetype}_Model.h5\")  # Save the model to a file\n",
    "\n",
    "cum_results.to_csv('Testing_output/Testing_103L_001DF_CumulativeResults.csv')\n",
    "all_each_results.to_csv('Testing_output/Testing_103L_001DF_EveryResults.csv')\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
